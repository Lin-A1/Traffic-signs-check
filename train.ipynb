{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95a9a81-ec30-4b84-9979-d10788874389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import DatasetFolder, ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import csv\n",
    "import torch.cuda.amp as amp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85dea5cf-84a9-4eec-9d64-eefb393f75f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "signs_dict = {\n",
    "    \"Speed limit (5km)\": 0,\n",
    "    \"Speed limit (15km)\": 1,\n",
    "    \"Speed limit (20km)\": 2,\n",
    "    \"Speed limit (30km)\": 3,\n",
    "    \"Speed limit (40km)\": 4,\n",
    "    \"Speed limit (50km)\": 5,\n",
    "    \"Speed limit (60km)\": 6,\n",
    "    \"Speed limit (70km)\": 7,\n",
    "    \"speed limit (80km)\": 8,\n",
    "    \"speed limit (100km)\": 9,\n",
    "    \"speed limit (120km)\": 10,\n",
    "    \"End of speed limit\": 11,\n",
    "    \"End of speed limit (50km)\": 12,\n",
    "    \"End of speed limit (80km)\": 13,\n",
    "    \"Dont overtake from Left\": 14,\n",
    "    \"No stopping\": 15,\n",
    "    \"No Uturn\": 16,\n",
    "    \"No Car\": 17,\n",
    "    \"No horn\": 18,\n",
    "    \"No entry\": 19,\n",
    "    \"No passage\": 20,\n",
    "    \"Dont Go Right\": 21,\n",
    "    \"Dont Go Left or Right\": 22,\n",
    "    \"Dont Go Left\": 23,\n",
    "    \"Dont Go straight\": 24,\n",
    "    \"Dont Go straight or Right\": 25,\n",
    "    \"Dont Go straight or left\": 26,\n",
    "    \"Go right or straight\": 27,\n",
    "    \"Go left or straight\": 28,\n",
    "    \"Village\": 29,\n",
    "    \"Uturn\": 30,\n",
    "    \"ZigZag Curve\": 31,\n",
    "    \"Bicycles crossing\": 32,\n",
    "    \"Keep Right\": 33,\n",
    "    \"Keep Left\": 34,\n",
    "    \"Roundabout mandatory\": 35,\n",
    "    \"Watch out for cars\": 36,\n",
    "    \"Slow down and give way\": 37,\n",
    "    \"Continuous detours\": 38,\n",
    "    \"Slow walking\": 39,\n",
    "    \"Horn\": 40,\n",
    "    \"Uphill steep slope\": 41,\n",
    "    \"Downhill steep slope\": 42,\n",
    "    \"Under Construction\": 43,\n",
    "    \"Heavy Vehicle Accidents\": 44,\n",
    "    \"Parking inspection\": 45,\n",
    "    \"Stop at intersection\": 46,\n",
    "    \"Train Crossing\": 47,\n",
    "    \"Fences\": 48,\n",
    "    \"Dangerous curve to the right\": 49,\n",
    "    \"Go Right\": 50,\n",
    "    \"Go Left or right\": 51,\n",
    "    \"Dangerous curve to the left\": 52,\n",
    "    \"Go Left\": 53,\n",
    "    \"Go straight\": 54,\n",
    "    \"Go straight or right\": 55,\n",
    "    \"Children crossing\": 56,\n",
    "    \"Care bicycles crossing\": 57,\n",
    "    \"Danger Ahead\": 58,\n",
    "    \"Traffic signals\": 59,\n",
    "    \"Zebra Crossing\": 60,\n",
    "    \"Road Divider\": 61\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a514ba9-4e77-4074-b550-fc6852c1c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResolutionTransform:\n",
    "    def __init__(self, model_path, scale=4):\n",
    "        self.sr = cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "        self.sr.readModel(model_path)\n",
    "        self.sr.setModel(\"espcn\", scale)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "        upsampled_img_cv = self.sr.upsample(img_cv)\n",
    "        upsampled_img = Image.fromarray(cv2.cvtColor(upsampled_img_cv, cv2.COLOR_BGR2RGB))\n",
    "        return upsampled_img\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)), # 高斯模糊\n",
    "    # SuperResolutionTransform(\"ESPCN_x4.pb\"),  # 超分辨率重构\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # 颜色调整\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    # transforms.Resize((50,50)),\n",
    "    transforms.Resize((224,224)),\n",
    "    # transforms.RandomHorizontalFlip(), # 随机翻转\n",
    "    transforms.RandomRotation(degrees=15),  # 随机旋转\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d3d5689-1778-4a99-aae0-2d86bf083006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据集类\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, label_dict=signs_dict):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.label_dict = label_dict\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for label_name, label_idx in label_dict.items():\n",
    "            label_dir = os.path.join(root_dir, label_name)\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                img_path = os.path.join(label_dir, img_name)\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(label_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc59c11c-31bf-4614-a0e7-9a116b7e88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入数据集\n",
    "dataset = CustomImageDataset(root_dir='./data/train_set/', transform=transform)\n",
    "\n",
    "# 定义训练集和验证集的比例\n",
    "train_ratio = 0.90  # 90%的数据用于训练\n",
    "val_ratio = 1 - train_ratio  # 剩下的用于验证\n",
    "\n",
    "# 计算划分后的样本数量\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# 划分数据集\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 创建训练集和验证集的数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68cb21d8-2448-4d91-a3ab-e8ff2ba3621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CA_Block(nn.Module):\n",
    "    def __init__(self, channel, h, w, reduction=16):\n",
    "        super(CA_Block, self).__init__()\n",
    " \n",
    "        self.h = h\n",
    "        self.w = w\n",
    " \n",
    "        self.avg_pool_x = nn.AdaptiveAvgPool2d((h, 1))\n",
    "        self.avg_pool_y = nn.AdaptiveAvgPool2d((1, w))\n",
    " \n",
    "        self.conv_1x1 = nn.Conv2d(in_channels=channel, out_channels=channel//reduction, kernel_size=1, stride=1, bias=False)\n",
    " \n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm2d(channel//reduction)\n",
    " \n",
    "        self.F_h = nn.Conv2d(in_channels=channel//reduction, out_channels=channel, kernel_size=1, stride=1, bias=False)\n",
    "        self.F_w = nn.Conv2d(in_channels=channel//reduction, out_channels=channel, kernel_size=1, stride=1, bias=False)\n",
    " \n",
    "        self.sigmoid_h = nn.Sigmoid()\n",
    "        self.sigmoid_w = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    " \n",
    "        x_h = self.avg_pool_x(x).permute(0, 1, 3, 2)\n",
    "        x_w = self.avg_pool_y(x)\n",
    " \n",
    "        x_cat_conv_relu = self.relu(self.conv_1x1(torch.cat((x_h, x_w), 3)))\n",
    " \n",
    "        x_cat_conv_split_h, x_cat_conv_split_w = x_cat_conv_relu.split([self.h, self.w], 3)\n",
    " \n",
    "        s_h = self.sigmoid_h(self.F_h(x_cat_conv_split_h.permute(0, 1, 3, 2)))\n",
    "        s_w = self.sigmoid_w(self.F_w(x_cat_conv_split_w))\n",
    " \n",
    "        out = x * s_h.expand_as(x) * s_w.expand_as(x)\n",
    " \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae955a1a-c01f-46c6-86f3-f4857c921294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\envs\\torch\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "E:\\envs\\torch\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class ResnetCA(nn.Module):\n",
    "    def __init__(self, model=models.resnet18(pretrained=False)):\n",
    "        super(ResnetCA,self).__init__()\n",
    "        self.resnet = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.ca = CA_Block(channel=512, h=7, w=7)\n",
    "        self.avg_pool = nn.Sequential(*list(model.children())[-2:])[0]\n",
    "        self.fc = nn.Linear(in_features=512, out_features=62, bias=True)\n",
    "    def forward(self, x):\n",
    "        x=self.resnet(x)\n",
    "        # x=x.view(-1,49,512)\n",
    "        x=self.ca(x)\n",
    "        x=x.view(-1,512,7,7)\n",
    "        x=self.avg_pool(x)\n",
    "        x=x.view(x.size(0), -1)\n",
    "        x=self.fc(x)\n",
    "        return x\n",
    "\n",
    "resnet_ca = ResnetCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d03f419-446c-4d20-b5c7-a5ccea488cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 7, 1]               0\n",
      "AdaptiveAvgPool2d-68            [-1, 512, 1, 7]               0\n",
      "           Conv2d-69            [-1, 32, 1, 14]          16,384\n",
      "             ReLU-70            [-1, 32, 1, 14]               0\n",
      "           Conv2d-71            [-1, 512, 7, 1]          16,384\n",
      "          Sigmoid-72            [-1, 512, 7, 1]               0\n",
      "           Conv2d-73            [-1, 512, 1, 7]          16,384\n",
      "          Sigmoid-74            [-1, 512, 1, 7]               0\n",
      "         CA_Block-75            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-76            [-1, 512, 1, 1]               0\n",
      "           Linear-77                   [-1, 62]          31,806\n",
      "================================================================\n",
      "Total params: 11,257,470\n",
      "Trainable params: 11,257,470\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 63.15\n",
      "Params size (MB): 42.94\n",
      "Estimated Total Size (MB): 106.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(resnet_ca,input_size=(3,224,224),device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bec989b-690a-42ed-bd2f-4e8e45a83945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    loss = running_loss / len(data_loader.dataset)\n",
    "    acc = correct / total\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    report = classification_report(all_labels, all_predictions,zero_division=0)\n",
    "    \n",
    "    return loss, acc, f1, report\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, device, epochs=10, logs='./logs/training_log.csv', save_model=False):\n",
    "    os.makedirs(os.path.dirname(logs), exist_ok=True)\n",
    "    classification_report_last = './logs/classification_report_last.txt'\n",
    "    classification_report_best = './logs/classification_report_best.txt'\n",
    "    \n",
    "    with open(logs, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Train Loss', 'Train Acc', 'Train F1', 'Val Loss', 'Val Acc', 'Val F1'])\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    best_val_acc = 0.0\n",
    "    total_steps = epochs\n",
    "    progress_bar = tqdm(total=total_steps, desc=\"epochs\")\n",
    "\n",
    "    scaler = amp.GradScaler()  # 创建混合精度训练的缩放器\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_train_predictions = []\n",
    "        all_train_labels = []\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with amp.autocast():  # 开始混合精度计算\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()  # 缩放损失值以进行反向传播\n",
    "            \n",
    "            scaler.step(optimizer)  # 调整模型参数\n",
    "            scaler.update()  # 更新缩放器\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_train_predictions.extend(predicted.cpu().tolist())\n",
    "            all_train_labels.extend(labels.cpu().tolist())\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = correct / total\n",
    "        train_f1 = f1_score(all_train_labels, all_train_predictions, average='macro')\n",
    "        \n",
    "        val_loss, val_acc, val_f1, val_report = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        with open(logs, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch + 1, train_loss, train_acc, train_f1, val_loss, val_acc, val_f1])\n",
    "        \n",
    "        # 保存最终模型的分类报告\n",
    "        with open(classification_report_last, 'w') as f:\n",
    "            f.write(val_report)\n",
    "        \n",
    "        # 保存最佳模型的分类报告\n",
    "        if val_acc >= best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_report = val_report\n",
    "            torch.save(model, 'model/best_model.pth')\n",
    "            \n",
    "            with open(classification_report_best, 'w') as f:\n",
    "                f.write(best_val_report)\n",
    "\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "877fdb7d-3cea-46c9-8086-15191b079665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数、优化器和设备\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet_ca.parameters(), lr=0.001)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38b5b90d-cad6-4c39-a244-b9276cb585a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs: 100%|██████████████████████████████████████████████████████████████████████| 100/100 [1:58:35<00:00, 71.15s/it]\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "train(resnet_ca.to(device), train_loader, val_loader, criterion, optimizer, device, epochs=100,save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58433c16-38c7-4064-aa51-ad412c73264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集类\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.image_files = os.listdir(folder_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.folder_path, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.image_files[idx]   \n",
    "\n",
    "# 定义预处理变换\n",
    "transform = transforms.Compose([\n",
    "    SuperResolutionTransform(\"ESPCN_x4.pb\"),  # 超分辨率重构\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # 颜色调整\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "    \n",
    "# 加载模型\n",
    "model_path = \"./model/best_model.pth\"\n",
    "model = torch.load(model_path)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 加载数据集\n",
    "test_dataset = ImageDataset(\"./data/test_set/unknow/\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 预测并保存结果到CSV\n",
    "results = []\n",
    "for images, filenames in test_loader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images.to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        results.append((filenames[0], predicted.item()))\n",
    "\n",
    "# 将结果保存到CSV\n",
    "df = pd.DataFrame(results, columns=[\"ImageID\", \"label\"])\n",
    "df.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949404db-8767-4e1f-b589-4bccb6af0a66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
